# å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººåç«¯ç³»ç»Ÿè¯¦ç»†æŠ€æœ¯æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®èƒŒæ™¯
å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººåç«¯ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½å¿ƒç†å’¨è¯¢å¹³å°ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å®‰å…¨ã€å¯é çš„å¿ƒç†å¥åº·æœåŠ¡ã€‚ç³»ç»Ÿé‡‡ç”¨ç°ä»£åŒ–çš„å¾®æœåŠ¡æ¶æ„ï¼Œé›†æˆäº†å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€å‘é‡æ•°æ®åº“ã€æ™ºèƒ½è·¯ç”±ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå®ç°äº†ä»ç”¨æˆ·äº¤äº’åˆ°ä¸“ä¸šå¿ƒç†å’¨è¯¢çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–å¤„ç†ã€‚

### æŠ€æœ¯æ„¿æ™¯
- **æ™ºèƒ½åŒ–**ï¼šåŸºäºDeepSeekå¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›é«˜è´¨é‡çš„å¿ƒç†å’¨è¯¢å¯¹è¯
- **å®‰å…¨æ€§**ï¼šå®ç°å±æœºè¯†åˆ«ä¸å¹²é¢„ï¼Œä¿éšœç”¨æˆ·å¿ƒç†å®‰å…¨
- **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒåŠŸèƒ½å¿«é€Ÿè¿­ä»£å’Œæ‰©å±•
- **é«˜æ€§èƒ½**ï¼šå‘é‡åŒ–æ£€ç´¢å’Œç¼“å­˜æœºåˆ¶ï¼Œç¡®ä¿å“åº”é€Ÿåº¦
- **ä¸“ä¸šæ€§**ï¼šèåˆå¿ƒç†å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œæä¾›å…±æƒ…å¼å¯¹è¯ä½“éªŒ

### æ ¸å¿ƒä»·å€¼
1. **24/7å¯ç”¨æ€§**ï¼šå…¨å¤©å€™æä¾›å¿ƒç†å¥åº·æ”¯æŒæœåŠ¡
2. **éšç§ä¿æŠ¤**ï¼šä¸¥æ ¼çš„æ•°æ®å®‰å…¨å’Œç”¨æˆ·éšç§ä¿æŠ¤æœºåˆ¶
3. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šåŸºäºç”¨æˆ·å†å²å’Œæƒ…æ„ŸçŠ¶æ€çš„ä¸ªæ€§åŒ–å“åº”
4. **ä¸“ä¸šæ ‡å‡†**ï¼šç¬¦åˆå¿ƒç†å’¨è¯¢è¡Œä¸šæ ‡å‡†å’Œä¼¦ç†è§„èŒƒ
5. **å¯è®¿é—®æ€§**ï¼šé™ä½å¿ƒç†å¥åº·æœåŠ¡çš„é—¨æ§›å’Œæˆæœ¬

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‰ç«¯ç”¨æˆ·ç•Œé¢å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    APIç½‘å…³å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  è®¤è¯æœåŠ¡    â”‚ â”‚  ä¼šè¯ç®¡ç†    â”‚ â”‚  èŠå¤©æœåŠ¡    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æ ¸å¿ƒä¸šåŠ¡é€»è¾‘å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ å¿ƒç†å’¨è¯¢æ§åˆ¶å™¨â”‚ â”‚  æ™ºèƒ½è·¯ç”±    â”‚ â”‚  å±æœºè¯†åˆ«    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  æ„å›¾åˆ†æ    â”‚ â”‚  æ–‡æ¡£æ£€ç´¢    â”‚ â”‚  ç»“æœé‡æ’    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æ•°æ®å¤„ç†å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  å‘é‡å­˜å‚¨    â”‚ â”‚  æ–‡æ¡£åŠ è½½å™¨  â”‚ â”‚  æµå¼æœåŠ¡    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æ•°æ®å­˜å‚¨å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  MySQLæ•°æ®åº“ â”‚ â”‚  Chromaå‘é‡åº“â”‚ â”‚  æ–‡ä»¶å­˜å‚¨    â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯æ ˆé€‰å‹

#### åç«¯æ¡†æ¶
- **FastAPI**: ç°ä»£åŒ–çš„Python Webæ¡†æ¶ï¼Œæ”¯æŒè‡ªåŠ¨APIæ–‡æ¡£ç”Ÿæˆ
- **SQLAlchemy**: å¼ºå¤§çš„ORMæ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ•°æ®åº“
- **Pydantic**: æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–åº“
- **LangChain**: å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶

#### æ•°æ®åº“æŠ€æœ¯
- **MySQL**: ä¸»æ•°æ®åº“ï¼Œå­˜å‚¨ç”¨æˆ·ä¿¡æ¯ã€ä¼šè¯è®°å½•ç­‰ç»“æ„åŒ–æ•°æ®
- **Chroma**: å‘é‡æ•°æ®åº“ï¼Œå­˜å‚¨æ–‡æ¡£åµŒå…¥å‘é‡ç”¨äºè¯­ä¹‰æ£€ç´¢
- **Redis**: ç¼“å­˜æ•°æ®åº“ï¼ˆè§„åˆ’ä¸­ï¼‰ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½

#### AI/MLæŠ€æœ¯
- **DeepSeek-Chat**: ä¸»è¦çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæä¾›å¯¹è¯ç”Ÿæˆèƒ½åŠ›
- **OpenAI Embeddings**: æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
- **HuggingFace Transformers**: æœ¬åœ°åµŒå…¥æ¨¡å‹æ”¯æŒ
- **jieba**: ä¸­æ–‡åˆ†è¯å·¥å…·ï¼Œç”¨äºæ–‡æœ¬å¤„ç†

#### å¼€å‘å·¥å…·
- **Poetry**: Pythonä¾èµ–ç®¡ç†
- **pytest**: å•å…ƒæµ‹è¯•æ¡†æ¶
- **Black**: ä»£ç æ ¼å¼åŒ–å·¥å…·
- **mypy**: é™æ€ç±»å‹æ£€æŸ¥

## æ ¸å¿ƒæ¨¡å—è¯¦ç»†è®¾è®¡

### 1. å‘é‡åŒ–ä¸ç´¢å¼•æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
å‘é‡åŒ–ä¸ç´¢å¼•æ¨¡å—æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œè´Ÿè´£å°†æ–‡æœ¬å†…å®¹è½¬æ¢ä¸ºé«˜ç»´å‘é‡è¡¨ç¤ºï¼Œå¹¶å»ºç«‹é«˜æ•ˆçš„ç´¢å¼•ç»“æ„ä»¥æ”¯æŒè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ã€‚

#### æŠ€æœ¯å®ç°

**æ–‡ä»¶ä½ç½®**: `core/vector_store.py`

**æ ¸å¿ƒåŠŸèƒ½**:
1. **åŠ¨æ€æ¨¡å‹åŠ è½½**: æ”¯æŒOpenAIå’ŒHuggingFaceä¸¤ç§åµŒå…¥æ¨¡å‹
2. **è®¾å¤‡è‡ªé€‚åº”**: è‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡ï¼ˆCUDA/MPS/CPUï¼‰
3. **å‘é‡å­˜å‚¨**: åŸºäºChromaçš„æŒä¹…åŒ–å‘é‡æ•°æ®åº“
4. **å®¹é”™æœºåˆ¶**: æ¨¡å‹åŠ è½½å¤±è´¥æ—¶çš„é™çº§ç­–ç•¥

**å…³é”®ä»£ç ç»“æ„**:
```python
def get_embedding_model():
    """åŠ¨æ€åŠ è½½åµŒå…¥æ¨¡å‹"""
    config = get_config()
    if config.embedding.provider == "openai":
        return OpenAIEmbeddings(model=config.embedding.model)
    else:
        return HuggingFaceEmbeddings(
            model_name=config.embedding.model,
            model_kwargs={'device': _get_device()}
        )

def get_vector_store():
    """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
    embedding_model = get_embedding_model()
    return Chroma(
        persist_directory=config.chroma.persist_directory,
        embedding_function=embedding_model
    )
```

**æ€§èƒ½ä¼˜åŒ–**:
- æ¨¡å‹ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤åŠ è½½
- æ‰¹é‡å‘é‡åŒ–å¤„ç†ï¼Œæå‡ååé‡
- å¼‚æ­¥å¤„ç†æ”¯æŒï¼Œå‡å°‘é˜»å¡æ—¶é—´

#### é…ç½®å‚æ•°
```yaml
embedding:
  provider: "openai"  # openai | huggingface
  model: "text-embedding-ada-002"
  batch_size: 100
  max_retries: 3

chroma:
  persist_directory: "./chroma_db"
  collection_name: "psychological_knowledge"
  distance_metric: "cosine"
```

### 2. æ··åˆæœç´¢æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
æ··åˆæœç´¢æ¨¡å—ç»“åˆäº†å‘é‡è¯­ä¹‰æœç´¢å’Œä¼ ç»Ÿå…³é”®è¯æœç´¢çš„ä¼˜åŠ¿ï¼Œé€šè¿‡å¤šç§æ£€ç´¢ç­–ç•¥çš„èåˆï¼Œæä¾›æ›´å‡†ç¡®å’Œå…¨é¢çš„æ–‡æ¡£æ£€ç´¢ç»“æœã€‚

#### å½“å‰å®ç°çŠ¶æ€
- âœ… **å‘é‡æœç´¢**: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–‡æ¡£æ£€ç´¢
- âš ï¸ **å…³é”®è¯æœç´¢**: éœ€è¦é›†æˆBM25ç®—æ³•
- âš ï¸ **ç»“æœèåˆ**: éœ€è¦å®ç°åŠ æƒèåˆç­–ç•¥

#### æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

**å‘é‡æœç´¢å®ç°**:
```python
def vector_search(query: str, k: int = 5) -> List[Document]:
    """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ–‡æ¡£æ£€ç´¢"""
    vector_store = get_vector_store()
    return vector_store.similarity_search(query, k=k)
```

**å…³é”®è¯æœç´¢è®¾è®¡**ï¼ˆå¾…å®ç°ï¼‰:
```python
from rank_bm25 import BM25Okapi

class KeywordSearcher:
    def __init__(self, documents: List[str]):
        self.tokenized_docs = [jieba.lcut(doc) for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_docs)
    
    def search(self, query: str, k: int = 5) -> List[Document]:
        """BM25å…³é”®è¯æœç´¢"""
        tokenized_query = jieba.lcut(query)
        scores = self.bm25.get_scores(tokenized_query)
        top_indices = np.argsort(scores)[::-1][:k]
        return [self.documents[i] for i in top_indices]
```

**èåˆç­–ç•¥è®¾è®¡**ï¼ˆå¾…å®ç°ï¼‰:
```python
def hybrid_search(query: str, k: int = 5, alpha: float = 0.7) -> List[Document]:
    """æ··åˆæœç´¢ï¼šå‘é‡æœç´¢ + å…³é”®è¯æœç´¢"""
    vector_results = vector_search(query, k=k*2)
    keyword_results = keyword_search(query, k=k*2)
    
    # åŠ æƒèåˆ
    fused_results = fuse_results(
        vector_results, keyword_results, 
        alpha=alpha, beta=1-alpha
    )
    return fused_results[:k]
```

#### ä¼˜åŒ–å»ºè®®
1. **å®ç°BM25ç®—æ³•**: é›†æˆrank_bm25åº“
2. **ç»“æœèåˆç­–ç•¥**: å®ç°RRFï¼ˆReciprocal Rank Fusionï¼‰ç®—æ³•
3. **æŸ¥è¯¢æ‰©å±•**: åŸºäºåŒä¹‰è¯å’Œç›¸å…³è¯çš„æŸ¥è¯¢æ‰©å±•
4. **ç¼“å­˜æœºåˆ¶**: çƒ­é—¨æŸ¥è¯¢ç»“æœç¼“å­˜

### 3. ç»“æœç²¾æ’æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
ç»“æœç²¾æ’æ¨¡å—å¯¹åˆæ­¥æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡ŒäºŒæ¬¡æ’åºï¼Œé€šè¿‡æ›´ç²¾ç»†çš„ç›¸å…³æ€§è®¡ç®—ï¼Œç¡®ä¿æœ€ç›¸å…³çš„å†…å®¹æ’åœ¨å‰é¢ã€‚

#### æŠ€æœ¯å®ç°

**æ–‡ä»¶ä½ç½®**: `core/psychological_tools.py`

**æ ¸å¿ƒç®—æ³•**:
```python
@tool
def rerank_documents(user_input: str, documents: List[str]) -> str:
    """ä½¿ç”¨TF-IDFå’Œä½™å¼¦ç›¸ä¼¼åº¦é‡æ’åºæ–‡æ¡£"""
    try:
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
        
        # ä¸­æ–‡åˆ†è¯
        def tokenize_chinese(text):
            return ' '.join(jieba.lcut(text))
        
        # å¤„ç†ç”¨æˆ·è¾“å…¥å’Œæ–‡æ¡£
        processed_input = tokenize_chinese(user_input)
        processed_docs = [tokenize_chinese(doc) for doc in documents]
        
        # TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer(
            stop_words=None,
            max_features=1000,
            ngram_range=(1, 2)
        )
        
        # æ„å»ºè¯­æ–™åº“
        corpus = [processed_input] + processed_docs
        tfidf_matrix = vectorizer.fit_transform(corpus)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        query_vector = tfidf_matrix[0]
        doc_vectors = tfidf_matrix[1:]
        similarities = cosine_similarity(query_vector, doc_vectors).flatten()
        
        # æ’åºå¹¶è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        ranked_indices = similarities.argsort()[::-1][:5]
        ranked_docs = [documents[i] for i in ranked_indices]
        
        return "\n\n".join(ranked_docs)
        
    except Exception as e:
        return f"é‡æ’åºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
```

#### ç®—æ³•ä¼˜åŠ¿
1. **TF-IDFç‰¹å¾æå–**: æ•è·è¯æ±‡é‡è¦æ€§å’Œæ–‡æ¡£ç‰¹å¾
2. **N-gramæ”¯æŒ**: è€ƒè™‘è¯æ±‡ç»„åˆçš„è¯­ä¹‰ä¿¡æ¯
3. **ä½™å¼¦ç›¸ä¼¼åº¦**: æ ‡å‡†åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
4. **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡æ–‡æœ¬çš„åˆ†è¯å¤„ç†

#### æ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®ç‡**: ç›¸å…³æ–‡æ¡£æ’åœ¨å‰ä½çš„æ¯”ä¾‹
- **å¬å›ç‡**: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å æ€»ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
- **å“åº”æ—¶é—´**: é‡æ’åºå¤„ç†çš„å¹³å‡è€—æ—¶
- **ç”¨æˆ·æ»¡æ„åº¦**: åŸºäºç”¨æˆ·åé¦ˆçš„è´¨é‡è¯„ä¼°

#### ä¼˜åŒ–æ–¹å‘
1. **æ·±åº¦å­¦ä¹ é‡æ’åº**: é›†æˆBERTç­‰é¢„è®­ç»ƒæ¨¡å‹
2. **å¤šç‰¹å¾èåˆ**: ç»“åˆè¯­ä¹‰ã€è¯­æ³•ã€æƒ…æ„Ÿç­‰å¤šç»´ç‰¹å¾
3. **ä¸ªæ€§åŒ–æ’åº**: åŸºäºç”¨æˆ·å†å²å’Œåå¥½çš„ä¸ªæ€§åŒ–æ’åº
4. **å®æ—¶å­¦ä¹ **: åŸºäºç”¨æˆ·åé¦ˆçš„åœ¨çº¿å­¦ä¹ æœºåˆ¶

### 4. æ•°æ®å¤„ç†æµæ°´çº¿æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
æ•°æ®å¤„ç†æµæ°´çº¿è´Ÿè´£å°†å„ç§æ ¼å¼çš„æ–‡æ¡£è½¬æ¢ä¸ºç³»ç»Ÿå¯å¤„ç†çš„æ ‡å‡†æ ¼å¼ï¼ŒåŒ…æ‹¬æ–‡æ¡£è§£æã€å†…å®¹æ¸…æ´—ã€æ–‡æœ¬åˆ†å—ç­‰é¢„å¤„ç†æ­¥éª¤ã€‚

#### å½“å‰å®ç°çŠ¶æ€
- âš ï¸ **é…ç½®æ”¯æŒ**: å·²æœ‰æ–‡æœ¬åˆ†å‰²é…ç½®å‚æ•°
- âŒ **å¤šæ ¼å¼æ”¯æŒ**: ç¼ºå°‘PDFã€Wordã€TXTç­‰æ ¼å¼å¤„ç†å™¨
- âŒ **å†…å®¹æ¸…æ´—**: ç¼ºå°‘æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–æµç¨‹
- âŒ **åˆ†å—ç­–ç•¥**: ç¼ºå°‘æ™ºèƒ½åˆ†å—ç®—æ³•

#### æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡

**é…ç½®å‚æ•°**:
```yaml
text_splitter:
  chunk_size: 1000
  chunk_overlap: 200
  separators: ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]
```

**æ–‡æ¡£å¤„ç†å™¨è®¾è®¡**:
```python
class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨åŸºç±»"""
    
    def process(self, file_path: str) -> List[Document]:
        """å¤„ç†æ–‡æ¡£å¹¶è¿”å›åˆ†å—ç»“æœ"""
        content = self.extract_content(file_path)
        cleaned_content = self.clean_content(content)
        chunks = self.split_content(cleaned_content)
        return [Document(page_content=chunk) for chunk in chunks]
    
    def extract_content(self, file_path: str) -> str:
        """æå–æ–‡æ¡£å†…å®¹"""
        raise NotImplementedError
    
    def clean_content(self, content: str) -> str:
        """æ¸…æ´—æ–‡æ¡£å†…å®¹"""
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,!?;:]', '', content)
        return content.strip()
    
    def split_content(self, content: str) -> List[str]:
        """æ™ºèƒ½åˆ†å—"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=self.config.separators
        )
        return splitter.split_text(content)

class PDFProcessor(DocumentProcessor):
    """PDFæ–‡æ¡£å¤„ç†å™¨"""
    
    def extract_content(self, file_path: str) -> str:
        import PyPDF2
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            content = ""
            for page in reader.pages:
                content += page.extract_text()
        return content

class WordProcessor(DocumentProcessor):
    """Wordæ–‡æ¡£å¤„ç†å™¨"""
    
    def extract_content(self, file_path: str) -> str:
        from docx import Document
        doc = Document(file_path)
        content = ""
        for paragraph in doc.paragraphs:
            content += paragraph.text + "\n"
        return content
```

**æµæ°´çº¿ç®¡ç†å™¨**:
```python
class DocumentPipeline:
    """æ–‡æ¡£å¤„ç†æµæ°´çº¿"""
    
    def __init__(self):
        self.processors = {
            '.pdf': PDFProcessor(),
            '.docx': WordProcessor(),
            '.txt': TextProcessor()
        }
    
    def process_document(self, file_path: str) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        file_ext = Path(file_path).suffix.lower()
        processor = self.processors.get(file_ext)
        
        if not processor:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        return processor.process(file_path)
    
    def process_directory(self, directory: str) -> List[Document]:
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ–‡æ¡£"""
        documents = []
        for file_path in Path(directory).rglob('*'):
            if file_path.is_file() and file_path.suffix in self.processors:
                try:
                    docs = self.process_document(str(file_path))
                    documents.extend(docs)
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
        return documents
```

#### å®ç°ä¼˜å…ˆçº§
1. **é«˜ä¼˜å…ˆçº§**: PDFå’ŒTXTå¤„ç†å™¨
2. **ä¸­ä¼˜å…ˆçº§**: Wordå’ŒMarkdownå¤„ç†å™¨
3. **ä½ä¼˜å…ˆçº§**: Excelå’ŒPowerPointå¤„ç†å™¨

### 5. æ™ºèƒ½è·¯ç”±æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
æ™ºèƒ½è·¯ç”±æ¨¡å—æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒè°ƒåº¦ä¸­å¿ƒï¼Œè´Ÿè´£åˆ†æç”¨æˆ·æ„å›¾ï¼Œå¹¶æ ¹æ®ä¸åŒçš„æ„å›¾ç±»å‹é€‰æ‹©ç›¸åº”çš„å¤„ç†æµç¨‹ã€‚

#### æŠ€æœ¯å®ç°

**æ–‡ä»¶ä½ç½®**: `core/psychological_controller.py`

**æ ¸å¿ƒæ¶æ„**:
```python
class PsychologicalChatController:
    """å¿ƒç†å’¨è¯¢èŠå¤©æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.tools = [
            analyze_intent,
            check_safety,
            retrieve_documents,
            rerank_documents,
            generate_answer
        ]
        self.llm = get_llm()
        self.agent = create_tool_calling_agent(self.llm, self.tools)
    
    async def process_message_stream(self, user_input: str, session_id: str):
        """æµå¼å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        try:
            # 1. æ„å›¾åˆ†æ
            yield self._create_stream_chunk("æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...")
            intent_result = await self._analyze_intent(user_input)
            
            # 2. å®‰å…¨æ£€æŸ¥
            if intent_result.get('intent') == 'crisis':
                yield self._create_stream_chunk("æ£€æµ‹åˆ°ç´§æ€¥æƒ…å†µï¼Œæ­£åœ¨å¤„ç†...")
                safety_result = await self._check_safety(user_input)
                if safety_result.get('risk_level') in ['high', 'medium']:
                    yield self._create_stream_chunk(safety_result.get('response'))
                    return
            
            # 3. æ–‡æ¡£æ£€ç´¢
            yield self._create_stream_chunk("æ­£åœ¨æœç´¢ç›¸å…³èµ„æ–™...")
            documents = await self._retrieve_documents(user_input, intent_result)
            
            # 4. æ–‡æ¡£é‡æ’åº
            if documents:
                yield self._create_stream_chunk("æ­£åœ¨ä¼˜åŒ–æœç´¢ç»“æœ...")
                ranked_docs = await self._rerank_documents(user_input, documents)
            else:
                ranked_docs = ""
            
            # 5. ç”Ÿæˆå›ç­”
            yield self._create_stream_chunk("æ­£åœ¨ç”Ÿæˆå›ç­”...")
            async for chunk in self._generate_answer_stream(
                user_input, intent_result, ranked_docs, session_id
            ):
                yield chunk
                
        except asyncio.TimeoutError:
            yield self._create_error_chunk("å¤„ç†è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
        except Exception as e:
            yield self._create_error_chunk(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
```

**æ„å›¾åˆ†ææµç¨‹**:
```python
@tool
def analyze_intent(user_input: str) -> str:
    """åˆ†æç”¨æˆ·æ„å›¾å’Œæƒ…æ„ŸçŠ¶æ€"""
    crisis_keywords = [
        "è‡ªæ€", "ç»“æŸç”Ÿå‘½", "ä¸æƒ³æ´»", "æ­»äº†ç®—äº†", "æ´»ç€æ²¡æ„æ€",
        "è‡ªæ®‹", "ä¼¤å®³è‡ªå·±", "æƒ³æ­»", "è½»ç”Ÿ", "äº†ç»“"
    ]
    
    consultation_keywords = [
        "æŠ‘éƒ", "ç„¦è™‘", "å‹åŠ›", "å¤±çœ ", "æƒ…ç»ª", "å¿ƒç†", "å’¨è¯¢",
        "æ²»ç–—", "ç—‡çŠ¶", "è¯Šæ–­", "è¯ç‰©", "å¿ƒç†åŒ»ç”Ÿ"
    ]
    
    knowledge_keywords = [
        "ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆåŠ", "æ–¹æ³•", "å»ºè®®",
        "åŸå› ", "ç—‡çŠ¶", "è¡¨ç°", "ç‰¹ç‚¹"
    ]
    
    # å±æœºæ£€æµ‹
    if any(keyword in user_input for keyword in crisis_keywords):
        return json.dumps({
            "intent": "crisis",
            "confidence": 0.9,
            "emotion": "negative",
            "urgency": "high"
        }, ensure_ascii=False)
    
    # å’¨è¯¢æ„å›¾æ£€æµ‹
    elif any(keyword in user_input for keyword in consultation_keywords):
        return json.dumps({
            "intent": "consultation",
            "confidence": 0.8,
            "emotion": "neutral",
            "urgency": "medium"
        }, ensure_ascii=False)
    
    # çŸ¥è¯†æŸ¥è¯¢æ„å›¾
    elif any(keyword in user_input for keyword in knowledge_keywords):
        return json.dumps({
            "intent": "knowledge",
            "confidence": 0.7,
            "emotion": "neutral",
            "urgency": "low"
        }, ensure_ascii=False)
    
    # é»˜è®¤ä¸ºé—²èŠ
    else:
        return json.dumps({
            "intent": "chat",
            "confidence": 0.6,
            "emotion": "neutral",
            "urgency": "low"
        }, ensure_ascii=False)
```

**è·¯ç”±å†³ç­–é€»è¾‘**:
1. **å±æœºè·¯ç”±**: ç«‹å³è§¦å‘å®‰å…¨æ£€æŸ¥å’Œå±æœºå¹²é¢„
2. **å’¨è¯¢è·¯ç”±**: æ£€ç´¢ä¸“ä¸šèµ„æ–™ï¼Œç”Ÿæˆä¸“ä¸šå»ºè®®
3. **çŸ¥è¯†è·¯ç”±**: æœç´¢çŸ¥è¯†åº“ï¼Œæä¾›ç§‘æ™®ä¿¡æ¯
4. **é—²èŠè·¯ç”±**: è¿›è¡Œæ—¥å¸¸å¯¹è¯ï¼Œå»ºç«‹ä¿¡ä»»å…³ç³»

#### æ€§èƒ½ä¼˜åŒ–
1. **å¼‚æ­¥å¤„ç†**: æ‰€æœ‰å·¥å…·è°ƒç”¨éƒ½é‡‡ç”¨å¼‚æ­¥æ¨¡å¼
2. **æµå¼å“åº”**: å®æ—¶è¿”å›å¤„ç†è¿›åº¦å’Œç»“æœ
3. **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜å¸¸è§æ„å›¾åˆ†æç»“æœ
4. **è¶…æ—¶æ§åˆ¶**: é˜²æ­¢é•¿æ—¶é—´é˜»å¡ç”¨æˆ·è¯·æ±‚

### 6. ç”¨æˆ·è®¤è¯ä¸ä¼šè¯ç®¡ç†

#### ç”¨æˆ·è®¤è¯æ¨¡å—

**æ–‡ä»¶ä½ç½®**: `api/auth.py`

**æ ¸å¿ƒåŠŸèƒ½**:
1. **ç”¨æˆ·æ³¨å†Œ**: é‚®ç®±å”¯ä¸€æ€§éªŒè¯ã€å¯†ç åŠ å¯†å­˜å‚¨
2. **ç”¨æˆ·ç™»å½•**: èº«ä»½éªŒè¯ã€JWTä»¤ç‰Œç”Ÿæˆ
3. **ä»¤ç‰Œç®¡ç†**: è®¿é—®ä»¤ç‰Œå’Œåˆ·æ–°ä»¤ç‰Œçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†
4. **å¯†ç å®‰å…¨**: bcryptåŠ å¯†ã€å¯†ç å¼ºåº¦éªŒè¯
5. **åŒ¿åæ”¯æŒ**: æ”¯æŒåŒ¿åç”¨æˆ·è®¿é—®éƒ¨åˆ†åŠŸèƒ½

**å®‰å…¨æœºåˆ¶**:
```python
# å¯†ç åŠ å¯†
def hash_password(password: str) -> str:
    """ä½¿ç”¨bcryptåŠ å¯†å¯†ç """
    salt = bcrypt.gensalt()
    return bcrypt.hashpw(password.encode('utf-8'), salt).decode('utf-8')

# JWTä»¤ç‰Œç”Ÿæˆ
def create_access_token(data: dict, expires_delta: timedelta = None):
    """åˆ›å»ºè®¿é—®ä»¤ç‰Œ"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=30)
    
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

# ç”¨æˆ·èº«ä»½éªŒè¯
async def get_current_user(token: str = Depends(oauth2_scheme)):
    """è·å–å½“å‰ç”¨æˆ·"""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    
    user = get_user_by_id(user_id)
    if user is None:
        raise credentials_exception
    return user
```

#### ä¼šè¯ç®¡ç†æ¨¡å—

**æ–‡ä»¶ä½ç½®**: `models/conversation.py`, `services/conversation_service.py`

**æ•°æ®æ¨¡å‹**:
```python
class Conversation(Base):
    """å¯¹è¯ä¼šè¯æ¨¡å‹"""
    __tablename__ = "conversations"
    
    conversation_id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String(36), ForeignKey("users.user_id"), nullable=True)
    title = Column(String(200), nullable=False, default="æ–°å¯¹è¯")
    message_count = Column(Integer, default=0)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_message_at = Column(DateTime, nullable=True)
    
    # å…³ç³»
    user = relationship("User", back_populates="conversations")
    messages = relationship("Message", back_populates="conversation", cascade="all, delete-orphan")

class Message(Base):
    """æ¶ˆæ¯æ¨¡å‹"""
    __tablename__ = "messages"
    
    message_id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    conversation_id = Column(String(36), ForeignKey("conversations.conversation_id"), nullable=False)
    role = Column(Enum(MessageRole), nullable=False)  # user, assistant
    content = Column(Text, nullable=False)
    model_name = Column(String(100), nullable=True)
    temperature = Column(Float, nullable=True)
    message_metadata = Column(JSON, nullable=True)
    feedback = Column(Integer, nullable=True)  # -1, 0, 1
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # å…³ç³»
    conversation = relationship("Conversation", back_populates="messages")
```

**æœåŠ¡å±‚åŠŸèƒ½**:
```python
class ConversationService:
    """ä¼šè¯ç®¡ç†æœåŠ¡"""
    
    def create_conversation(self, user_id: str = None, title: str = "æ–°å¯¹è¯") -> Conversation:
        """åˆ›å»ºæ–°ä¼šè¯"""
        conversation = Conversation(
            user_id=user_id,
            title=title
        )
        self.db.add(conversation)
        self.db.commit()
        return conversation
    
    def get_user_conversations(self, user_id: str, limit: int = 20) -> List[Conversation]:
        """è·å–ç”¨æˆ·ä¼šè¯åˆ—è¡¨"""
        return self.db.query(Conversation)\
            .filter(Conversation.user_id == user_id)\
            .filter(Conversation.is_active == True)\
            .order_by(Conversation.last_message_at.desc())\
            .limit(limit)\
            .all()
    
    def add_message(self, conversation_id: str, role: MessageRole, 
                   content: str, metadata: dict = None) -> Message:
        """æ·»åŠ æ¶ˆæ¯åˆ°ä¼šè¯"""
        message = Message(
            conversation_id=conversation_id,
            role=role,
            content=content,
            message_metadata=metadata
        )
        
        # æ›´æ–°ä¼šè¯ä¿¡æ¯
        conversation = self.get_conversation(conversation_id)
        conversation.message_count += 1
        conversation.last_message_at = datetime.utcnow()
        
        self.db.add(message)
        self.db.commit()
        return message
    
    def delete_conversation(self, conversation_id: str, user_id: str) -> bool:
        """åˆ é™¤ä¼šè¯ï¼ˆè½¯åˆ é™¤ï¼‰"""
        conversation = self.db.query(Conversation)\
            .filter(Conversation.conversation_id == conversation_id)\
            .filter(Conversation.user_id == user_id)\
            .first()
        
        if conversation:
            conversation.is_active = False
            self.db.commit()
            return True
        return False
```

### 7. å±æœºè¯†åˆ«ä¸å¹²é¢„æ¨¡å—

#### æ¨¡å—æ¦‚è¿°
å±æœºè¯†åˆ«ä¸å¹²é¢„æ¨¡å—æ˜¯ç³»ç»Ÿçš„å®‰å…¨ä¿éšœæ ¸å¿ƒï¼Œè´Ÿè´£å®æ—¶ç›‘æµ‹ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ï¼Œè¯†åˆ«æ½œåœ¨çš„è‡ªä¼¤æˆ–è‡ªæ€é£é™©ï¼Œå¹¶æä¾›åŠæ—¶çš„å¹²é¢„æªæ–½ã€‚

#### æŠ€æœ¯å®ç°

**æ–‡ä»¶ä½ç½®**: `core/psychological_tools.py`

**é£é™©è¯„ä¼°ç®—æ³•**:
```python
@tool
def check_safety(user_input: str) -> str:
    """æ£€æŸ¥ç”¨æˆ·è¾“å…¥çš„å®‰å…¨æ€§ï¼Œè¯†åˆ«è‡ªä¼¤æˆ–è‡ªæ€é£é™©"""
    
    # é«˜é£é™©å…³é”®è¯
    high_risk_keywords = [
        "æƒ³æ­»", "è‡ªæ€", "ç»“æŸç”Ÿå‘½", "ä¸æƒ³æ´»äº†", "æ­»äº†ç®—äº†",
        "è·³æ¥¼", "ä¸ŠåŠ", "å‰²è…•", "æœæ¯’", "äº†ç»“è‡ªå·±"
    ]
    
    # ä¸­é£é™©å…³é”®è¯
    medium_risk_keywords = [
        "æ´»ç€æ²¡æ„æ€", "ç”Ÿæ— å¯æ‹", "ç»æœ›", "ç—›è‹¦", "ç…ç†¬",
        "è§£è„±", "ä¸€äº†ç™¾äº†", "æ¶ˆå¤±", "ç¦»å¼€è¿™ä¸ªä¸–ç•Œ"
    ]
    
    # è‡ªä¼¤å…³é”®è¯
    self_harm_keywords = [
        "è‡ªæ®‹", "ä¼¤å®³è‡ªå·±", "å‰²ä¼¤", "çƒ«ä¼¤", "æ’å¢™",
        "è‡ªè™", "æƒ©ç½šè‡ªå·±", "è®©è‡ªå·±å—ä¼¤"
    ]
    
    user_input_lower = user_input.lower()
    
    # é«˜é£é™©æ£€æµ‹
    high_risk_count = sum(1 for keyword in high_risk_keywords 
                         if keyword in user_input_lower)
    
    # ä¸­é£é™©æ£€æµ‹
    medium_risk_count = sum(1 for keyword in medium_risk_keywords 
                           if keyword in user_input_lower)
    
    # è‡ªä¼¤é£é™©æ£€æµ‹
    self_harm_count = sum(1 for keyword in self_harm_keywords 
                         if keyword in user_input_lower)
    
    # é£é™©ç­‰çº§åˆ¤å®š
    if high_risk_count >= 2 or (high_risk_count >= 1 and medium_risk_count >= 1):
        risk_level = "high"
        response = generate_crisis_response("high")
    elif high_risk_count >= 1 or medium_risk_count >= 2 or self_harm_count >= 1:
        risk_level = "medium"
        response = generate_crisis_response("medium")
    elif medium_risk_count >= 1:
        risk_level = "low"
        response = generate_crisis_response("low")
    else:
        risk_level = "safe"
        response = "ç”¨æˆ·è¾“å…¥å®‰å…¨"
    
    return json.dumps({
        "risk_level": risk_level,
        "high_risk_count": high_risk_count,
        "medium_risk_count": medium_risk_count,
        "self_harm_count": self_harm_count,
        "response": response,
        "timestamp": datetime.now().isoformat()
    }, ensure_ascii=False)

def generate_crisis_response(risk_level: str) -> str:
    """ç”Ÿæˆå±æœºå¹²é¢„å“åº”"""
    
    if risk_level == "high":
        return """
æˆ‘éå¸¸æ‹…å¿ƒæ‚¨ç°åœ¨çš„çŠ¶æ€ã€‚æ‚¨çš„ç”Ÿå‘½å¾ˆå®è´µï¼Œè¯·ä¸è¦æ”¾å¼ƒã€‚

ğŸš¨ ç´§æ€¥æ±‚åŠ©çƒ­çº¿ï¼š
â€¢ å…¨å›½å¿ƒç†å±æœºå¹²é¢„çƒ­çº¿ï¼š400-161-9995
â€¢ åŒ—äº¬å±æœºå¹²é¢„çƒ­çº¿ï¼š400-161-9995
â€¢ ä¸Šæµ·å¿ƒç†æ´åŠ©çƒ­çº¿ï¼š021-64383562

è¯·ç«‹å³è”ç³»ä»¥ä¸Šçƒ­çº¿æˆ–å‰å¾€æœ€è¿‘çš„åŒ»é™¢æ€¥è¯Šç§‘ã€‚
å¦‚æœæ‚¨èº«è¾¹æœ‰ä¿¡ä»»çš„æœ‹å‹æˆ–å®¶äººï¼Œä¹Ÿè¯·ç«‹å³è”ç³»ä»–ä»¬ã€‚

æ‚¨å¹¶ä¸å­¤å•ï¼Œæ€»æœ‰äººæ„¿æ„å¸®åŠ©æ‚¨åº¦è¿‡è¿™ä¸ªå›°éš¾æ—¶æœŸã€‚
        """.strip()
    
    elif risk_level == "medium":
        return """
æˆ‘èƒ½æ„Ÿå—åˆ°æ‚¨ç°åœ¨å¾ˆç—›è‹¦ï¼Œè¿™ç§æ„Ÿå—ä¸€å®šå¾ˆéš¾å—ã€‚

ğŸ’™ ä¸“ä¸šæ”¯æŒèµ„æºï¼š
â€¢ å¿ƒç†å’¨è¯¢çƒ­çº¿ï¼š400-161-9995
â€¢ åœ¨çº¿å¿ƒç†å’¨è¯¢å¹³å°ï¼šå£¹å¿ƒç†ã€ç®€å•å¿ƒç†
â€¢ å½“åœ°å¿ƒç†å¥åº·ä¸­å¿ƒ

è¯·è®°ä½ï¼š
â€¢ è¿™ç§ç—›è‹¦çš„æ„Ÿè§‰æ˜¯æš‚æ—¶çš„
â€¢ å¯»æ±‚ä¸“ä¸šå¸®åŠ©æ˜¯å‹‡æ•¢çš„è¡¨ç°
â€¢ æ‚¨å€¼å¾—è¢«å…³çˆ±å’Œæ”¯æŒ

å¦‚æœæƒ…å†µç´§æ€¥ï¼Œè¯·ä¸è¦çŠ¹è±«ç«‹å³æ‹¨æ‰“æ€¥æ•‘ç”µè¯120ã€‚
        """.strip()
    
    elif risk_level == "low":
        return """
æˆ‘æ³¨æ„åˆ°æ‚¨å¯èƒ½æ­£åœ¨ç»å†ä¸€äº›å›°éš¾ã€‚è¿™å¾ˆæ­£å¸¸ï¼Œæ¯ä¸ªäººéƒ½ä¼šæœ‰ä½è½çš„æ—¶å€™ã€‚

ğŸŒŸ å»ºè®®æ‚¨ï¼š
â€¢ ä¸ä¿¡ä»»çš„æœ‹å‹æˆ–å®¶äººäº¤æµ
â€¢ å°è¯•ä¸€äº›æ”¾æ¾æŠ€å·§ï¼Œå¦‚æ·±å‘¼å¸æˆ–å†¥æƒ³
â€¢ ä¿æŒè§„å¾‹çš„ä½œæ¯å’Œé€‚é‡è¿åŠ¨
â€¢ å¦‚æœæŒç»­æ„Ÿåˆ°å›°æ‰°ï¼Œè€ƒè™‘å¯»æ±‚ä¸“ä¸šå¿ƒç†å’¨è¯¢

è®°ä½ï¼Œå¯»æ±‚å¸®åŠ©æ˜¯åŠ›é‡çš„ä½“ç°ï¼Œä¸æ˜¯è½¯å¼±ã€‚
        """.strip()
    
    return "è¯·æ³¨æ„æ‚¨çš„å¿ƒç†å¥åº·çŠ¶æ€ã€‚"
```

#### å¹²é¢„ç­–ç•¥

**å³æ—¶å¹²é¢„**:
1. **é«˜é£é™©**: ç«‹å³æä¾›ç´§æ€¥è”ç³»æ–¹å¼ï¼Œå»ºè®®å¯»æ±‚ä¸“ä¸šå¸®åŠ©
2. **ä¸­é£é™©**: æä¾›å¿ƒç†æ”¯æŒèµ„æºï¼Œé¼“åŠ±å¯»æ±‚ä¸“ä¸šå’¨è¯¢
3. **ä½é£é™©**: æä¾›è‡ªåŠ©å»ºè®®å’Œé¢„é˜²æ€§æŒ‡å¯¼

**æŒç»­ç›‘æŠ¤**:
```python
class CrisisMonitor:
    """å±æœºç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.risk_history = {}
        self.alert_threshold = 3  # è¿ç»­é£é™©æ£€æµ‹é˜ˆå€¼
    
    def track_user_risk(self, user_id: str, risk_level: str):
        """è·Ÿè¸ªç”¨æˆ·é£é™©çŠ¶æ€"""
        if user_id not in self.risk_history:
            self.risk_history[user_id] = []
        
        self.risk_history[user_id].append({
            'risk_level': risk_level,
            'timestamp': datetime.now()
        })
        
        # ä¿ç•™æœ€è¿‘10æ¬¡è®°å½•
        self.risk_history[user_id] = self.risk_history[user_id][-10:]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§å¹²é¢„
        if self._should_escalate(user_id):
            self._escalate_intervention(user_id)
    
    def _should_escalate(self, user_id: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‡çº§å¹²é¢„"""
        recent_risks = self.risk_history.get(user_id, [])[-3:]
        return len([r for r in recent_risks if r['risk_level'] in ['high', 'medium']]) >= 2
    
    def _escalate_intervention(self, user_id: str):
        """å‡çº§å¹²é¢„æªæ–½"""
        # å‘é€è­¦æŠ¥ç»™ç®¡ç†å‘˜
        # è‡ªåŠ¨è”ç³»ç´§æ€¥è”ç³»äºº
        # è®°å½•åˆ°å±æœºæ—¥å¿—
        logger.critical(f"ç”¨æˆ· {user_id} éœ€è¦ç´§æ€¥å¹²é¢„")
```

#### è´¨é‡ä¿è¯
1. **å‡†ç¡®æ€§éªŒè¯**: å®šæœŸè¯„ä¼°å…³é”®è¯æ£€æµ‹çš„å‡†ç¡®ç‡
2. **è¯¯æŠ¥æ§åˆ¶**: å¹³è¡¡æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ï¼Œå‡å°‘è¯¯æŠ¥
3. **å“åº”æ—¶æ•ˆ**: ç¡®ä¿å±æœºå“åº”åœ¨ç§’çº§å®Œæˆ
4. **ä¸“ä¸šå®¡æ ¸**: å®šæœŸç”±å¿ƒç†ä¸“å®¶å®¡æ ¸å¹²é¢„ç­–ç•¥

### 8. æµå¼å“åº”ä¸æ€§èƒ½ä¼˜åŒ–

#### æµå¼æœåŠ¡æ¨¡å—

**æ–‡ä»¶ä½ç½®**: `services/streaming_service.py`

**æ ¸å¿ƒå®ç°**:
```python
class StreamingService:
    """æµå¼å“åº”æœåŠ¡"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=get_config().deepseek.api_key,
            base_url="https://api.deepseek.com"
        )
    
    async def stream_chat_completion(self, messages: List[dict], 
                                   model: str = "deepseek-chat",
                                   temperature: float = 0.7) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        try:
            response = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                stream=True,
                max_tokens=2000
            )
            
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield self._format_sse_data(content)
                    
        except Exception as e:
            yield self._format_sse_data(f"æµå¼å“åº”é”™è¯¯: {str(e)}")
    
    def _format_sse_data(self, data: str) -> str:
        """æ ¼å¼åŒ–SSEæ•°æ®"""
        return f"data: {json.dumps({'content': data}, ensure_ascii=False)}\n\n"
    
    async def stream_psychological_chat(self, user_input: str, 
                                      chat_history: List[dict] = None) -> AsyncGenerator[str, None]:
        """å¿ƒç†å’¨è¯¢æµå¼å¯¹è¯"""
        system_prompt = """
æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¿ƒç†å­¦çŸ¥è¯†å’Œå’¨è¯¢ç»éªŒã€‚
è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä¿æŒä¸“ä¸šã€æ¸©æš–ã€éè¯„åˆ¤çš„æ€åº¦
2. è¿ç”¨å…±æƒ…æŠ€å·§ï¼Œç†è§£å’Œåæ˜ ç”¨æˆ·çš„æƒ…æ„Ÿ
3. æä¾›ä¸“ä¸šçš„å¿ƒç†å­¦è§è§£å’Œå»ºè®®
4. åœ¨å¿…è¦æ—¶å»ºè®®å¯»æ±‚ä¸“ä¸šå¸®åŠ©
5. ä¿æŠ¤ç”¨æˆ·éšç§å’Œå°Šä¸¥
        """
        
        messages = [{"role": "system", "content": system_prompt}]
        
        if chat_history:
            messages.extend(chat_history)
        
        messages.append({"role": "user", "content": user_input})
        
        async for chunk in self.stream_chat_completion(messages):
            yield chunk
```

#### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. è¿æ¥æ± ç®¡ç†**:
```python
class ConnectionPoolManager:
    """è¿æ¥æ± ç®¡ç†å™¨"""
    
    def __init__(self):
        self.pools = {
            'database': self._create_db_pool(),
            'redis': self._create_redis_pool(),
            'http': self._create_http_pool()
        }
    
    def _create_db_pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        return create_engine(
            DATABASE_URL,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600
        )
    
    def _create_redis_pool(self):
        """åˆ›å»ºRedisè¿æ¥æ± """
        return redis.ConnectionPool(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            max_connections=50
        )
```

**2. ç¼“å­˜ç­–ç•¥**:
```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = redis.Redis(connection_pool=redis_pool)
        self.local_cache = {}
        self.cache_ttl = {
            'intent_analysis': 300,  # 5åˆ†é’Ÿ
            'document_search': 600,  # 10åˆ†é’Ÿ
            'user_profile': 1800,    # 30åˆ†é’Ÿ
        }
    
    async def get_cached_intent(self, user_input: str) -> Optional[dict]:
        """è·å–ç¼“å­˜çš„æ„å›¾åˆ†æç»“æœ"""
        cache_key = f"intent:{hashlib.md5(user_input.encode()).hexdigest()}"
        
        # å…ˆæ£€æŸ¥æœ¬åœ°ç¼“å­˜
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]
        
        # å†æ£€æŸ¥Redisç¼“å­˜
        cached_data = await self.redis_client.get(cache_key)
        if cached_data:
            result = json.loads(cached_data)
            self.local_cache[cache_key] = result
            return result
        
        return None
    
    async def cache_intent_result(self, user_input: str, result: dict):
        """ç¼“å­˜æ„å›¾åˆ†æç»“æœ"""
        cache_key = f"intent:{hashlib.md5(user_input.encode()).hexdigest()}"
        ttl = self.cache_ttl['intent_analysis']
        
        # å­˜å‚¨åˆ°Redis
        await self.redis_client.setex(
            cache_key, 
            ttl, 
            json.dumps(result, ensure_ascii=False)
        )
        
        # å­˜å‚¨åˆ°æœ¬åœ°ç¼“å­˜
        self.local_cache[cache_key] = result
```

**3. å¼‚æ­¥å¤„ç†ä¼˜åŒ–**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncProcessor:
    """å¼‚æ­¥å¤„ç†å™¨"""
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=10)
        self.semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°
    
    async def process_with_timeout(self, coro, timeout: float = 30.0):
        """å¸¦è¶…æ—¶çš„å¼‚æ­¥å¤„ç†"""
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            raise TimeoutError(f"å¤„ç†è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
    
    async def parallel_process(self, tasks: List[Callable]) -> List[Any]:
        """å¹¶è¡Œå¤„ç†å¤šä¸ªä»»åŠ¡"""
        async with self.semaphore:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
```

## æ•°æ®åº“è®¾è®¡

### æ•°æ®åº“æ¶æ„

#### MySQLä¸»æ•°æ®åº“

**ç”¨æˆ·è¡¨ (users)**:
```sql
CREATE TABLE users (
    user_id VARCHAR(36) PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    last_login_at TIMESTAMP NULL,
    current_emotion VARCHAR(50) NULL,
    emotion_history JSON NULL,
    emotion_updated_at TIMESTAMP NULL,
    INDEX idx_email (email),
    INDEX idx_username (username),
    INDEX idx_active (is_active)
);
```

**ä¼šè¯è¡¨ (conversations)**:
```sql
CREATE TABLE conversations (
    conversation_id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(36) NULL,
    title VARCHAR(200) NOT NULL DEFAULT 'æ–°å¯¹è¯',
    message_count INT DEFAULT 0,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    last_message_at TIMESTAMP NULL,
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE SET NULL,
    INDEX idx_user_id (user_id),
    INDEX idx_active (is_active),
    INDEX idx_last_message (last_message_at)
);
```

**æ¶ˆæ¯è¡¨ (messages)**:
```sql
CREATE TABLE messages (
    message_id VARCHAR(36) PRIMARY KEY,
    conversation_id VARCHAR(36) NOT NULL,
    role ENUM('user', 'assistant') NOT NULL,
    content TEXT NOT NULL,
    model_name VARCHAR(100) NULL,
    temperature FLOAT NULL,
    message_metadata JSON NULL,
    feedback INT NULL CHECK (feedback IN (-1, 0, 1)),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(conversation_id) ON DELETE CASCADE,
    INDEX idx_conversation_id (conversation_id),
    INDEX idx_role (role),
    INDEX idx_created_at (created_at),
    INDEX idx_feedback (feedback)
);
```

**å±æœºè®°å½•è¡¨ (crisis_logs)**:
```sql
CREATE TABLE crisis_logs (
    log_id VARCHAR(36) PRIMARY KEY,
    user_id VARCHAR(36) NULL,
    conversation_id VARCHAR(36) NULL,
    risk_level ENUM('low', 'medium', 'high') NOT NULL,
    trigger_content TEXT NOT NULL,
    intervention_response TEXT NOT NULL,
    resolved BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    resolved_at TIMESTAMP NULL,
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE SET NULL,
    FOREIGN KEY (conversation_id) REFERENCES conversations(conversation_id) ON DELETE SET NULL,
    INDEX idx_user_id (user_id),
    INDEX idx_risk_level (risk_level),
    INDEX idx_resolved (resolved),
    INDEX idx_created_at (created_at)
);
```

#### Chromaå‘é‡æ•°æ®åº“

**é›†åˆé…ç½®**:
```python
chroma_config = {
    "collection_name": "psychological_knowledge",
    "embedding_function": "text-embedding-ada-002",
    "distance_metric": "cosine",
    "metadata_fields": [
        "source",      # æ–‡æ¡£æ¥æº
        "category",    # åˆ†ç±»ï¼ˆæŠ‘éƒã€ç„¦è™‘ã€æ²»ç–—æ–¹æ³•ç­‰ï¼‰
        "authority",   # æƒå¨æ€§è¯„åˆ†
        "last_updated", # æœ€åæ›´æ–°æ—¶é—´
        "language"     # è¯­è¨€
    ]
}
```

**æ–‡æ¡£ç»“æ„**:
```json
{
    "id": "doc_001",
    "content": "æŠ‘éƒç—‡æ˜¯ä¸€ç§å¸¸è§çš„å¿ƒç†ç–¾ç—…...",
    "embedding": [0.1, 0.2, ...],
    "metadata": {
        "source": "DSM-5",
        "category": "æŠ‘éƒç—‡",
        "authority": 0.95,
        "last_updated": "2024-01-15",
        "language": "zh-CN"
    }
}
```

### æ•°æ®åº“ä¼˜åŒ–ç­–ç•¥

#### ç´¢å¼•ä¼˜åŒ–
```sql
-- å¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_user_conversations ON conversations(user_id, is_active, last_message_at);
CREATE INDEX idx_conversation_messages ON messages(conversation_id, created_at);
CREATE INDEX idx_crisis_monitoring ON crisis_logs(user_id, risk_level, created_at);

-- å…¨æ–‡ç´¢å¼•æ”¯æŒå†…å®¹æœç´¢
CREATE FULLTEXT INDEX idx_message_content ON messages(content);
```

#### åˆ†åŒºç­–ç•¥
```sql
-- æŒ‰æ—¶é—´åˆ†åŒºæ¶ˆæ¯è¡¨
ALTER TABLE messages PARTITION BY RANGE (YEAR(created_at)) (
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p_future VALUES LESS THAN MAXVALUE
);
```

#### æ•°æ®å½’æ¡£
```python
class DataArchiver:
    """æ•°æ®å½’æ¡£å™¨"""
    
    async def archive_old_conversations(self, days_old: int = 365):
        """å½’æ¡£æ—§ä¼šè¯æ•°æ®"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        # æŸ¥æ‰¾éœ€è¦å½’æ¡£çš„ä¼šè¯
        old_conversations = await self.db.execute(
            select(Conversation)
            .where(Conversation.last_message_at < cutoff_date)
            .where(Conversation.is_active == False)
        )
        
        for conversation in old_conversations:
            # å¯¼å‡ºåˆ°å½’æ¡£å­˜å‚¨
            await self.export_to_archive(conversation)
            # ä»ä¸»æ•°æ®åº“åˆ é™¤
            await self.db.delete(conversation)
        
        await self.db.commit()
    
    async def export_to_archive(self, conversation: Conversation):
        """å¯¼å‡ºä¼šè¯åˆ°å½’æ¡£å­˜å‚¨"""
        archive_data = {
            'conversation': conversation.to_dict(),
            'messages': [msg.to_dict() for msg in conversation.messages],
            'archived_at': datetime.now().isoformat()
        }
        
        # å­˜å‚¨åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆå¦‚AWS S3ã€é˜¿é‡Œäº‘OSSç­‰ï¼‰
        archive_key = f"archives/{conversation.conversation_id}.json"
        await self.object_storage.put(archive_key, json.dumps(archive_data))
```

## APIæ¥å£è®¾è®¡

### RESTful APIè§„èŒƒ

#### è®¤è¯æ¥å£
```python
# POST /api/auth/register
{
    "username": "user123",
    "email": "user@example.com",
    "password": "SecurePass123!"
}

# Response
{
    "user_id": "uuid",
    "username": "user123",
    "email": "user@example.com",
    "access_token": "jwt_token",
    "refresh_token": "refresh_token",
    "token_type": "bearer",
    "expires_in": 1800
}

# POST /api/auth/login
{
    "email": "user@example.com",
    "password": "SecurePass123!"
}

# POST /api/auth/refresh
{
    "refresh_token": "refresh_token"
}

# POST /api/auth/logout
# Headers: Authorization: Bearer <access_token>
```

#### ä¼šè¯ç®¡ç†æ¥å£
```python
# GET /api/conversations
# Headers: Authorization: Bearer <access_token>
# Response
{
    "conversations": [
        {
            "conversation_id": "uuid",
            "title": "å…³äºç„¦è™‘çš„å’¨è¯¢",
            "message_count": 15,
            "last_message_at": "2024-01-15T10:30:00Z",
            "created_at": "2024-01-15T09:00:00Z"
        }
    ],
    "total": 10,
    "page": 1,
    "per_page": 20
}

# POST /api/conversations
{
    "title": "æ–°çš„å¿ƒç†å’¨è¯¢"
}

# GET /api/conversations/{conversation_id}/messages
# Response
{
    "messages": [
        {
            "message_id": "uuid",
            "role": "user",
            "content": "æˆ‘æœ€è¿‘æ„Ÿåˆ°å¾ˆç„¦è™‘",
            "created_at": "2024-01-15T10:00:00Z"
        },
        {
            "message_id": "uuid",
            "role": "assistant",
            "content": "æˆ‘ç†è§£æ‚¨çš„æ„Ÿå—...",
            "created_at": "2024-01-15T10:01:00Z",
            "model_name": "deepseek-chat",
            "temperature": 0.7
        }
    ]
}

# DELETE /api/conversations/{conversation_id}
```

#### èŠå¤©æ¥å£
```python
# POST /api/chat
{
    "message": "æˆ‘æœ€è¿‘æ€»æ˜¯å¤±çœ ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ",
    "conversation_id": "uuid",  # å¯é€‰
    "stream": true  # æ˜¯å¦æµå¼å“åº”
}

# éæµå¼å“åº”
{
    "response": "å¤±çœ æ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜...",
    "conversation_id": "uuid",
    "message_id": "uuid",
    "intent": "consultation",
    "emotion": "concerned",
    "processing_time": 2.5
}

# æµå¼å“åº” (Server-Sent Events)
data: {"type": "start", "message": "æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."}
data: {"type": "progress", "message": "æ­£åœ¨æœç´¢ç›¸å…³èµ„æ–™..."}
data: {"type": "content", "content": "å¤±çœ ç¡®å®æ˜¯ä¸€ä¸ª"}
data: {"type": "content", "content": "å¸¸è§çš„ç¡çœ é—®é¢˜..."}
data: {"type": "end", "conversation_id": "uuid", "message_id": "uuid"}
```

#### å¥åº·æ£€æŸ¥æ¥å£
```python
# GET /api/health
{
    "status": "healthy",
    "timestamp": "2024-01-15T10:00:00Z",
    "version": "1.0.0",
    "services": {
        "database": "connected",
        "vector_store": "connected",
        "llm_service": "available",
        "cache": "connected"
    }
}

# GET /api/metrics
{
    "active_users": 150,
    "total_conversations": 1250,
    "messages_today": 3500,
    "average_response_time": 2.3,
    "system_load": 0.65
}
```

### APIå®‰å…¨ä¸é™æµ

#### è¯·æ±‚é™æµ
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/api/chat")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡è¯·æ±‚
async def chat_endpoint(request: Request, chat_request: ChatRequest):
    """èŠå¤©æ¥å£"""
    pass

@app.post("/api/auth/login")
@limiter.limit("5/minute")  # ç™»å½•é™åˆ¶æ›´ä¸¥æ ¼
async def login_endpoint(request: Request, login_data: LoginRequest):
    """ç™»å½•æ¥å£"""
    pass
```

#### è¾“å…¥éªŒè¯
```python
from pydantic import BaseModel, validator, Field
from typing import Optional

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    conversation_id: Optional[str] = Field(None, regex=r'^[0-9a-f-]{36}$')
    stream: bool = Field(default=False)
    
    @validator('message')
    def validate_message(cls, v):
        if not v.strip():
            raise ValueError('æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()

class RegisterRequest(BaseModel):
    username: str = Field(..., min_length=3, max_length=50, regex=r'^[a-zA-Z0-9_]+$')
    email: str = Field(..., regex=r'^[\w\.-]+@[\w\.-]+\.\w+$')
    password: str = Field(..., min_length=8, max_length=128)
    
    @validator('password')
    def validate_password(cls, v):
        if not any(c.isupper() for c in v):
            raise ValueError('å¯†ç å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå¤§å†™å­—æ¯')
        if not any(c.islower() for c in v):
            raise ValueError('å¯†ç å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå°å†™å­—æ¯')
        if not any(c.isdigit() for c in v):
            raise ValueError('å¯†ç å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªæ•°å­—')
        return v
```

## éƒ¨ç½²ä¸è¿ç»´

### å®¹å™¨åŒ–éƒ¨ç½²

#### Dockerfile
```dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY pyproject.toml poetry.lock ./

# å®‰è£…Poetry
RUN pip install poetry

# é…ç½®Poetry
RUN poetry config virtualenvs.create false

# å®‰è£…Pythonä¾èµ–
RUN poetry install --no-dev

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### Docker Compose
```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql+pymysql://user:password@db:3306/psychological_chat
      - REDIS_URL=redis://redis:6379/0
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis
    volumes:
      - ./chroma_db:/app/chroma_db
      - ./logs:/app/logs
    restart: unless-stopped
    
  db:
    image: mysql:8.0
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=psychological_chat
      - MYSQL_USER=user
      - MYSQL_PASSWORD=password
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped

volumes:
  mysql_data:
  redis_data:
```

### ç›‘æ§ä¸æ—¥å¿—

#### æ—¥å¿—é…ç½®
```python
import logging
from logging.handlers import RotatingFileHandler
import structlog

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

# æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
file_handler = RotatingFileHandler(
    'logs/app.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
file_handler.setLevel(logging.INFO)

# é”™è¯¯æ—¥å¿—å¤„ç†å™¨
error_handler = RotatingFileHandler(
    'logs/error.log',
    maxBytes=10*1024*1024,
    backupCount=5
)
error_handler.setLevel(logging.ERROR)

# é…ç½®æ ¹æ—¥å¿—å™¨
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, error_handler],
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = structlog.get_logger()
```

#### æ€§èƒ½ç›‘æ§
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Active WebSocket connections')
LLM_RESPONSE_TIME = Histogram('llm_response_time_seconds', 'LLM response time')
CRISIS_ALERTS = Counter('crisis_alerts_total', 'Total crisis alerts triggered')

# ä¸­é—´ä»¶
@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    REQUEST_DURATION.observe(duration)
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    return response

# ç›‘æ§ç«¯ç‚¹
@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

### å®‰å…¨é…ç½®

#### HTTPSé…ç½®
```nginx
server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name your-domain.com;
    
    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;
    
    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";
    
    location / {
        proxy_pass http://app:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocketæ”¯æŒ
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
    
    # é™æ€æ–‡ä»¶ç¼“å­˜
    location /static/ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

#### ç¯å¢ƒå˜é‡ç®¡ç†
```bash
# .env.production
DATABASE_URL=mysql+pymysql://user:password@db:3306/psychological_chat
REDIS_URL=redis://redis:6379/0
DEEPSEEK_API_KEY=your_deepseek_api_key
OPENAI_API_KEY=your_openai_api_key
SECRET_KEY=your_secret_key_here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7
ENVIRONMENT=production
LOG_LEVEL=INFO
CORS_ORIGINS=https://your-frontend-domain.com
MAX_UPLOAD_SIZE=10485760  # 10MB
RATE_LIMIT_PER_MINUTE=60
```

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
import pytest
from fastapi.testclient import TestClient
from unittest.mock import Mock, patch
from main import app
from core.psychological_tools import analyze_intent, check_safety

client = TestClient(app)

class TestPsychologicalTools:
    """å¿ƒç†å·¥å…·æµ‹è¯•ç±»"""
    
    def test_analyze_intent_crisis(self):
        """æµ‹è¯•å±æœºæ„å›¾è¯†åˆ«"""
        user_input = "æˆ‘ä¸æƒ³æ´»äº†ï¼Œæƒ³è¦è‡ªæ€"
        result = analyze_intent(user_input)
        
        import json
        parsed_result = json.loads(result)
        
        assert parsed_result["intent"] == "crisis"
        assert parsed_result["confidence"] >= 0.8
        assert parsed_result["urgency"] == "high"
    
    def test_analyze_intent_consultation(self):
        """æµ‹è¯•å’¨è¯¢æ„å›¾è¯†åˆ«"""
        user_input = "æˆ‘æœ€è¿‘æ„Ÿåˆ°å¾ˆæŠ‘éƒï¼Œéœ€è¦å¿ƒç†å’¨è¯¢"
        result = analyze_intent(user_input)
        
        import json
        parsed_result = json.loads(result)
        
        assert parsed_result["intent"] == "consultation"
        assert parsed_result["confidence"] >= 0.7
    
    def test_check_safety_high_risk(self):
        """æµ‹è¯•é«˜é£é™©å®‰å…¨æ£€æŸ¥"""
        user_input = "æˆ‘æƒ³æ­»ï¼Œå‡†å¤‡è·³æ¥¼è‡ªæ€"
        result = check_safety(user_input)
        
        import json
        parsed_result = json.loads(result)
        
        assert parsed_result["risk_level"] == "high"
        assert "ç´§æ€¥æ±‚åŠ©çƒ­çº¿" in parsed_result["response"]
    
    def test_check_safety_safe(self):
        """æµ‹è¯•å®‰å…¨è¾“å…¥"""
        user_input = "ä»Šå¤©å¤©æ°”å¾ˆå¥½"
        result = check_safety(user_input)
        
        import json
        parsed_result = json.loads(result)
        
        assert parsed_result["risk_level"] == "safe"

class TestAPI:
    """APIæ¥å£æµ‹è¯•ç±»"""
    
    def test_health_check(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
        response = client.get("/api/health")
        assert response.status_code == 200
        assert response.json()["status"] == "healthy"
    
    @patch('core.psychological_controller.PsychologicalChatController')
    def test_chat_endpoint(self, mock_controller):
        """æµ‹è¯•èŠå¤©æ¥å£"""
        mock_controller.return_value.process_message_stream.return_value = [
            {"type": "content", "content": "æµ‹è¯•å“åº”"}
        ]
        
        response = client.post("/api/chat", json={
            "message": "ä½ å¥½",
            "stream": False
        })
        
        assert response.status_code == 200
    
    def test_register_user(self):
        """æµ‹è¯•ç”¨æˆ·æ³¨å†Œ"""
        response = client.post("/api/auth/register", json={
            "username": "testuser",
            "email": "test@example.com",
            "password": "TestPass123!"
        })
        
        assert response.status_code in [200, 201, 409]  # 409è¡¨ç¤ºç”¨æˆ·å·²å­˜åœ¨

@pytest.fixture
def mock_vector_store():
    """æ¨¡æ‹Ÿå‘é‡å­˜å‚¨"""
    with patch('core.vector_store.get_vector_store') as mock:
        mock_store = Mock()
        mock_store.similarity_search.return_value = [
            Mock(page_content="æµ‹è¯•æ–‡æ¡£å†…å®¹1"),
            Mock(page_content="æµ‹è¯•æ–‡æ¡£å†…å®¹2")
        ]
        mock.return_value = mock_store
        yield mock_store

@pytest.fixture
def mock_llm():
    """æ¨¡æ‹Ÿå¤§è¯­è¨€æ¨¡å‹"""
    with patch('core.llm.get_llm') as mock:
        mock_llm = Mock()
        mock_llm.invoke.return_value = Mock(content="æµ‹è¯•LLMå“åº”")
        mock.return_value = mock_llm
        yield mock_llm
```

### é›†æˆæµ‹è¯•

```python
import pytest
import asyncio
from httpx import AsyncClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from models.base import Base
from database import get_db
from main import app

# æµ‹è¯•æ•°æ®åº“é…ç½®
TEST_DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(TEST_DATABASE_URL, connect_args={"check_same_thread": False})
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def override_get_db():
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db

@pytest.fixture(scope="session")
def setup_test_db():
    """è®¾ç½®æµ‹è¯•æ•°æ®åº“"""
    Base.metadata.create_all(bind=engine)
    yield
    Base.metadata.drop_all(bind=engine)

@pytest.mark.asyncio
class TestIntegration:
    """é›†æˆæµ‹è¯•ç±»"""
    
    async def test_complete_chat_flow(self, setup_test_db):
        """æµ‹è¯•å®Œæ•´çš„èŠå¤©æµç¨‹"""
        async with AsyncClient(app=app, base_url="http://test") as ac:
            # 1. æ³¨å†Œç”¨æˆ·
            register_response = await ac.post("/api/auth/register", json={
                "username": "testuser",
                "email": "test@example.com",
                "password": "TestPass123!"
            })
            
            if register_response.status_code == 201:
                user_data = register_response.json()
                token = user_data["access_token"]
            else:
                # ç”¨æˆ·å·²å­˜åœ¨ï¼Œå°è¯•ç™»å½•
                login_response = await ac.post("/api/auth/login", json={
                    "email": "test@example.com",
                    "password": "TestPass123!"
                })
                assert login_response.status_code == 200
                token = login_response.json()["access_token"]
            
            headers = {"Authorization": f"Bearer {token}"}
            
            # 2. åˆ›å»ºä¼šè¯
            conversation_response = await ac.post(
                "/api/conversations",
                json={"title": "æµ‹è¯•ä¼šè¯"},
                headers=headers
            )
            assert conversation_response.status_code == 201
            conversation_id = conversation_response.json()["conversation_id"]
            
            # 3. å‘é€æ¶ˆæ¯
            chat_response = await ac.post("/api/chat", json={
                "message": "æˆ‘æ„Ÿåˆ°å¾ˆç„¦è™‘",
                "conversation_id": conversation_id,
                "stream": False
            }, headers=headers)
            
            assert chat_response.status_code == 200
            chat_data = chat_response.json()
            assert "response" in chat_data
            assert chat_data["conversation_id"] == conversation_id
            
            # 4. è·å–ä¼šè¯å†å²
            messages_response = await ac.get(
                f"/api/conversations/{conversation_id}/messages",
                headers=headers
            )
            assert messages_response.status_code == 200
            messages = messages_response.json()["messages"]
            assert len(messages) >= 2  # ç”¨æˆ·æ¶ˆæ¯ + åŠ©æ‰‹å›å¤
```

### æ€§èƒ½æµ‹è¯•

```python
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class PerformanceTest:
    """æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def single_request(self, session: aiohttp.ClientSession, endpoint: str, data: dict = None):
        """å•ä¸ªè¯·æ±‚æµ‹è¯•"""
        start_time = time.time()
        
        try:
            if data:
                async with session.post(f"{self.base_url}{endpoint}", json=data) as response:
                    await response.text()
                    status = response.status
            else:
                async with session.get(f"{self.base_url}{endpoint}") as response:
                    await response.text()
                    status = response.status
            
            duration = time.time() - start_time
            self.results.append({
                'endpoint': endpoint,
                'duration': duration,
                'status': status,
                'success': status == 200
            })
            
        except Exception as e:
            duration = time.time() - start_time
            self.results.append({
                'endpoint': endpoint,
                'duration': duration,
                'status': 0,
                'success': False,
                'error': str(e)
            })
    
    async def load_test(self, endpoint: str, concurrent_users: int = 10, 
                       requests_per_user: int = 10, data: dict = None):
        """è´Ÿè½½æµ‹è¯•"""
        connector = aiohttp.TCPConnector(limit=100)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            
            for _ in range(concurrent_users):
                for _ in range(requests_per_user):
                    task = self.single_request(session, endpoint, data)
                    tasks.append(task)
            
            await asyncio.gather(*tasks)
    
    def analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.results:
            return {}
        
        successful_requests = [r for r in self.results if r['success']]
        failed_requests = [r for r in self.results if not r['success']]
        
        if successful_requests:
            durations = [r['duration'] for r in successful_requests]
            avg_duration = sum(durations) / len(durations)
            max_duration = max(durations)
            min_duration = min(durations)
            
            # è®¡ç®—ç™¾åˆ†ä½æ•°
            sorted_durations = sorted(durations)
            p95_index = int(len(sorted_durations) * 0.95)
            p99_index = int(len(sorted_durations) * 0.99)
            
            return {
                'total_requests': len(self.results),
                'successful_requests': len(successful_requests),
                'failed_requests': len(failed_requests),
                'success_rate': len(successful_requests) / len(self.results) * 100,
                'average_response_time': avg_duration,
                'max_response_time': max_duration,
                'min_response_time': min_duration,
                'p95_response_time': sorted_durations[p95_index] if p95_index < len(sorted_durations) else max_duration,
                'p99_response_time': sorted_durations[p99_index] if p99_index < len(sorted_durations) else max_duration,
                'requests_per_second': len(successful_requests) / sum(durations) if sum(durations) > 0 else 0
            }
        
        return {
            'total_requests': len(self.results),
            'successful_requests': 0,
            'failed_requests': len(failed_requests),
            'success_rate': 0
        }

# è¿è¡Œæ€§èƒ½æµ‹è¯•
async def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    test = PerformanceTest()
    
    # æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£
    print("æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£...")
    await test.load_test("/api/health", concurrent_users=20, requests_per_user=50)
    health_results = test.analyze_results()
    print(f"å¥åº·æ£€æŸ¥æ¥å£æµ‹è¯•ç»“æœ: {health_results}")
    
    # é‡ç½®ç»“æœ
    test.results = []
    
    # æµ‹è¯•èŠå¤©æ¥å£
    print("æµ‹è¯•èŠå¤©æ¥å£...")
    chat_data = {"message": "ä½ å¥½", "stream": False}
    await test.load_test("/api/chat", concurrent_users=10, requests_per_user=20, data=chat_data)
    chat_results = test.analyze_results()
    print(f"èŠå¤©æ¥å£æµ‹è¯•ç»“æœ: {chat_results}")

if __name__ == "__main__":
    asyncio.run(run_performance_tests())
```

## æœªæ¥å‘å±•è§„åˆ’

### çŸ­æœŸç›®æ ‡ï¼ˆ1-3ä¸ªæœˆï¼‰

1. **å®Œå–„æ ¸å¿ƒåŠŸèƒ½**
   - å®ç°BM25å…³é”®è¯æœç´¢ç®—æ³•
   - å®Œå–„æ··åˆæœç´¢ç»“æœèåˆç­–ç•¥
   - ä¼˜åŒ–æ–‡æ¡£å¤„ç†æµæ°´çº¿ï¼Œæ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼
   - å¢å¼ºå±æœºè¯†åˆ«ç®—æ³•çš„å‡†ç¡®æ€§

2. **æ€§èƒ½ä¼˜åŒ–**
   - é›†æˆRedisç¼“å­˜ç³»ç»Ÿ
   - å®ç°æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
   - æ·»åŠ CDNæ”¯æŒé™æ€èµ„æºåŠ é€Ÿ
   - ä¼˜åŒ–å‘é‡æ£€ç´¢æ€§èƒ½

3. **ç”¨æˆ·ä½“éªŒæå‡**
   - å®ç°å®æ—¶æ‰“å­—æŒ‡ç¤ºå™¨
   - æ·»åŠ æ¶ˆæ¯çŠ¶æ€æ˜¾ç¤ºï¼ˆå‘é€ä¸­ã€å·²é€è¾¾ã€å·²è¯»ï¼‰
   - æ”¯æŒæ¶ˆæ¯ç¼–è¾‘å’Œåˆ é™¤åŠŸèƒ½
   - å®ç°ä¼šè¯å¯¼å‡ºåŠŸèƒ½

### ä¸­æœŸç›®æ ‡ï¼ˆ3-6ä¸ªæœˆï¼‰

1. **æ™ºèƒ½åŒ–å¢å¼º**
   - é›†æˆæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œå®æ—¶ç›‘æµ‹ç”¨æˆ·æƒ…ç»ªå˜åŒ–
   - å®ç°ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿï¼Œæ ¹æ®ç”¨æˆ·å†å²æä¾›å®šåˆ¶åŒ–å»ºè®®
   - æ·»åŠ å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
   - å®ç°æ™ºèƒ½è¯é¢˜å¼•å¯¼å’Œè½¬æ¢

2. **ä¸“ä¸šåŒ–åŠŸèƒ½**
   - é›†æˆæ ‡å‡†åŒ–å¿ƒç†è¯„ä¼°é‡è¡¨ï¼ˆPHQ-9ã€GAD-7ç­‰ï¼‰
   - å®ç°å¿ƒç†å¥åº·æŠ¥å‘Šç”Ÿæˆ
   - æ·»åŠ ä¸“ä¸šæœ¯è¯­è§£é‡Šå’Œç§‘æ™®åŠŸèƒ½
   - æ”¯æŒå¤šç§å¿ƒç†æ²»ç–—æ–¹æ³•ï¼ˆCBTã€DBTç­‰ï¼‰

3. **å¤šæ¨¡æ€æ”¯æŒ**
   - æ”¯æŒè¯­éŸ³è¾“å…¥å’Œè¾“å‡º
   - æ·»åŠ å›¾ç‰‡å’Œæ–‡æ¡£ä¸Šä¼ åŠŸèƒ½
   - å®ç°è§†é¢‘é€šè¯é¢„çº¦åŠŸèƒ½
   - æ”¯æŒè¡¨æƒ…ç¬¦å·å’Œè´´çº¸

### é•¿æœŸç›®æ ‡ï¼ˆ6-12ä¸ªæœˆï¼‰

1. **å¹³å°åŒ–å‘å±•**
   - å¼€å‘ç§»åŠ¨ç«¯åº”ç”¨ï¼ˆiOS/Androidï¼‰
   - å®ç°å¤šç§Ÿæˆ·æ¶æ„ï¼Œæ”¯æŒæœºæ„éƒ¨ç½²
   - æ·»åŠ ç®¡ç†åå°å’Œæ•°æ®åˆ†æé¢æ¿
   - æ”¯æŒç¬¬ä¸‰æ–¹é›†æˆå’ŒAPIå¼€æ”¾

2. **AIèƒ½åŠ›å‡çº§**
   - è®­ç»ƒä¸“é—¨çš„å¿ƒç†å¥åº·é¢†åŸŸæ¨¡å‹
   - å®ç°å¤šè¯­è¨€æ”¯æŒï¼ˆè‹±è¯­ã€æ—¥è¯­ç­‰ï¼‰
   - æ·»åŠ é¢„æµ‹æ€§åˆ†æï¼Œæå‰è¯†åˆ«å¿ƒç†å¥åº·é£é™©
   - é›†æˆæœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯

3. **ç”Ÿæ€ç³»ç»Ÿå»ºè®¾**
   - å»ºç«‹ä¸“ä¸šå¿ƒç†å’¨è¯¢å¸ˆç½‘ç»œ
   - å®ç°åœ¨çº¿é¢„çº¦å’Œæ”¯ä»˜ç³»ç»Ÿ
   - æ·»åŠ ç¤¾åŒºåŠŸèƒ½ï¼Œæ”¯æŒç”¨æˆ·äº’åŠ©
   - å»ºç«‹å¿ƒç†å¥åº·çŸ¥è¯†åº“å’Œèµ„æºä¸­å¿ƒ

### æŠ€æœ¯å€ºåŠ¡æ¸…ç†

1. **ä»£ç è´¨é‡æå‡**
   - å¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–ç‡è‡³90%ä»¥ä¸Š
   - å®ç°è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥å’Œè´¨é‡æ£€æŸ¥
   - é‡æ„é—ç•™ä»£ç ï¼Œæé«˜å¯ç»´æŠ¤æ€§
   - å»ºç«‹å®Œå–„çš„æ–‡æ¡£ä½“ç³»

2. **å®‰å…¨æ€§åŠ å¼º**
   - å®ç°ç«¯åˆ°ç«¯åŠ å¯†
   - æ·»åŠ æ•°æ®è„±æ•å’ŒåŒ¿ååŒ–åŠŸèƒ½
   - å®Œå–„è®¿é—®æ§åˆ¶å’Œæƒé™ç®¡ç†
   - é€šè¿‡å®‰å…¨è®¤è¯ï¼ˆå¦‚ISO 27001ï¼‰

3. **å¯æ‰©å±•æ€§ä¼˜åŒ–**
   - å®ç°å¾®æœåŠ¡æ¶æ„æ‹†åˆ†
   - æ·»åŠ æœåŠ¡ç½‘æ ¼å’ŒAPIç½‘å…³
   - æ”¯æŒå®¹å™¨ç¼–æ’å’Œè‡ªåŠ¨æ‰©ç¼©å®¹
   - å®ç°å¤šåŒºåŸŸéƒ¨ç½²å’Œç¾å¤‡

### å•†ä¸šåŒ–è€ƒè™‘

1. **ç›ˆåˆ©æ¨¡å¼**
   - å…è´¹åŸºç¡€ç‰ˆ + ä»˜è´¹é«˜çº§åŠŸèƒ½
   - ä¼ä¸šç‰ˆæœ¬å’Œå®šåˆ¶åŒ–æœåŠ¡
   - ä¸“ä¸šå’¨è¯¢å¸ˆå¹³å°æŠ½æˆ
   - æ•°æ®æ´å¯Ÿå’ŒæŠ¥å‘ŠæœåŠ¡

2. **åˆè§„è¦æ±‚**
   - åŒ»ç–—å™¨æ¢°è®¤è¯ç”³è¯·
   - æ•°æ®ä¿æŠ¤æ³•è§„éµå¾ªï¼ˆGDPRã€CCPAç­‰ï¼‰
   - å¿ƒç†å¥åº·è¡Œä¸šæ ‡å‡†è®¤è¯
   - éšç§æ”¿ç­–å’Œç”¨æˆ·åè®®å®Œå–„

3. **å¸‚åœºæ¨å¹¿**
   - ä¸åŒ»ç–—æœºæ„å’Œå­¦æ ¡åˆä½œ
   - å‚ä¸å¿ƒç†å¥åº·å…¬ç›Šæ´»åŠ¨
   - å»ºç«‹å“ç‰ŒçŸ¥ååº¦å’Œç”¨æˆ·ä¿¡ä»»
   - æ”¶é›†ç”¨æˆ·åé¦ˆå’Œæ”¹è¿›å»ºè®®

## æ€»ç»“

å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººåç«¯ç³»ç»Ÿæ˜¯ä¸€ä¸ªå¤æ‚è€Œé‡è¦çš„é¡¹ç›®ï¼Œå®ƒç»“åˆäº†å…ˆè¿›çš„AIæŠ€æœ¯ã€ä¸¥æ ¼çš„å®‰å…¨æ ‡å‡†å’Œä¸“ä¸šçš„å¿ƒç†å­¦çŸ¥è¯†ã€‚é€šè¿‡æ¨¡å—åŒ–çš„è®¾è®¡ã€å®Œå–„çš„æµ‹è¯•ç­–ç•¥å’ŒæŒç»­çš„ä¼˜åŒ–æ”¹è¿›ï¼Œç³»ç»Ÿèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›å®‰å…¨ã€ä¸“ä¸šã€ä¸ªæ€§åŒ–çš„å¿ƒç†å¥åº·æœåŠ¡ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

1. **æŠ€æœ¯å…ˆè¿›æ€§**: é‡‡ç”¨æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ£€ç´¢æŠ€æœ¯
2. **å®‰å…¨å¯é æ€§**: å®Œå–„çš„å±æœºè¯†åˆ«å’Œå¹²é¢„æœºåˆ¶
3. **ä¸“ä¸šæ€§**: åŸºäºå¿ƒç†å­¦ä¸“ä¸šçŸ¥è¯†çš„å¯¹è¯è®¾è®¡
4. **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–æ¶æ„æ”¯æŒåŠŸèƒ½å¿«é€Ÿè¿­ä»£
5. **ç”¨æˆ·ä½“éªŒ**: æµå¼å“åº”å’Œä¸ªæ€§åŒ–æœåŠ¡

### å…³é”®æŒ‘æˆ˜

1. **å‡†ç¡®æ€§**: ç¡®ä¿AIå›å¤çš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
2. **å®‰å…¨æ€§**: æœ‰æ•ˆè¯†åˆ«å’Œå¤„ç†å¿ƒç†å±æœºæƒ…å†µ
3. **éšç§ä¿æŠ¤**: ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·æ•æ„Ÿä¿¡æ¯
4. **æ€§èƒ½ä¼˜åŒ–**: åœ¨é«˜å¹¶å‘ä¸‹ä¿æŒå“åº”é€Ÿåº¦
5. **æŒç»­æ”¹è¿›**: åŸºäºç”¨æˆ·åé¦ˆä¸æ–­ä¼˜åŒ–ç³»ç»Ÿ

é€šè¿‡æŒç»­çš„æŠ€æœ¯åˆ›æ–°å’Œä¸“ä¸šåŒ–å‘å±•ï¼Œè¿™ä¸ªç³»ç»Ÿæœ‰æœ›æˆä¸ºå¿ƒç†å¥åº·é¢†åŸŸçš„é‡è¦å·¥å…·ï¼Œä¸ºæ›´å¤šéœ€è¦å¸®åŠ©çš„äººæä¾›åŠæ—¶ã€ä¸“ä¸šçš„å¿ƒç†æ”¯æŒæœåŠ¡ã€‚